{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6588f3",
   "metadata": {},
   "source": [
    "***GENERATED CODE FOR dp PIPELINE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be051a",
   "metadata": {},
   "source": [
    "***DON'T EDIT THIS CODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708da9d0",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO READ DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67179554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class HDFSConnector:\n",
    "\n",
    "    def fetch(spark, config):\n",
    "        ################### INPUT HADOOP HOST PORT TO CONNECT WITH ###############################\n",
    "        hdfs_server = str(os.environ['HDFS_SERVER'])\n",
    "        hdfs_port = int(os.environ['HDFS_PORT'])\n",
    "        df = spark.read.options(header='true', inferschema='true').csv(\n",
    "            f\"hdfs://{hdfs_server}:{hdfs_port}{eval(config)['url']}\", header='true')\n",
    "        display(df.limit(2).toPandas())\n",
    "        return df\n",
    "\n",
    "    def put(df, spark, config):\n",
    "        return df.write.format('csv').options(header='true' if eval(config)[\"is_header\"] == \"Use Header Line\" else 'false',\n",
    "                                              delimiter=eval(config)[\"delimiter\"]).save((\"%s %s\") % (datetime.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")+\"_\", eval(config)['url']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b8dc9",
   "metadata": {},
   "source": [
    "***OPERATION FUNCTIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from dask.dataframe import from_pandas\n",
    "import json\n",
    "\n",
    "\n",
    "def replaceValues(df, functionsData, applyOn):\n",
    "    for columnData in applyOn:\n",
    "        for functionData in functionsData:\n",
    "            if functionData['replaceType'] == 'value':\n",
    "                if functionData['replaceValueType'] == 'min':\n",
    "                    minValue = df[columnData['columnName']].min().compute()\n",
    "                    df[columnData['columnName']] = df[columnData['columnName']].replace(\n",
    "                        toReplace, minValue)\n",
    "                elif functionData['replaceValueType'] == 'max':\n",
    "                    maxValue = df[columnData['columnName']].max().compute()\n",
    "                    df[columnData['columnName']] = df[columnData['columnName']].replace(\n",
    "                        toReplace, maxValue)\n",
    "                else:\n",
    "                    df[columnData['columnName']] = df[columnData['columnName']].replace(\n",
    "                        toReplace, functionData['ReplaceWith'])\n",
    "            elif functionData['replaceType'] == 'none':\n",
    "                if functionData['replaceValueType'] == 'min':\n",
    "                    minValue = df[columnData['columnName']].min().compute()\n",
    "                    df[columnData['columnName']\n",
    "                       ] = df[columnData['columnName']].replace(\"\", minValue)\n",
    "                    df[columnData['columnName']\n",
    "                       ] = df[columnData['columnName']].fillna(minValue)\n",
    "                elif functionData['replaceValueType'] == 'max':\n",
    "                    maxValue = df[columnData['columnName']].max().compute()\n",
    "                    df[columnData['columnName']\n",
    "                       ] = df[columnData['columnName']].replace(\"\", maxValue)\n",
    "                    df[columnData['columnName']\n",
    "                       ] = df[columnData['columnName']].fillna(maxValue)\n",
    "                else:\n",
    "                    df[columnData['columnName']] = df[columnData['columnName']].replace(\n",
    "                        \"\", functionData['ReplaceWith'])\n",
    "                    df[columnData['columnName']] = df[columnData['columnName']].fillna(\n",
    "                        functionData['ReplaceWith'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def runDataCleansing(sparkDf, spark, config):\n",
    "    configObj = json.loads(config)\n",
    "    sparkDf.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    df = from_pandas((sparkDf.toPandas()), npartitions=5)\n",
    "    functionList = configObj['functionsApplied']\n",
    "    Data_Cleansing_Methods = {\"replaceBy\": replaceValues,\n",
    "                              \"formula\": calculateFormula,\n",
    "                              \"aggregate\": aggregation,\n",
    "                              \"converttostringtype\": changeToString,\n",
    "                              \"editname\": renameColumns}\n",
    "    for function in functionList:\n",
    "        function['functionName']\n",
    "        df = Data_Cleansing_Methods[function['functionName']](df, function['functionsData'],\n",
    "                                                              function['applyOn'])\n",
    "    sparkDf = spark.createDataFrame(df.compute())\n",
    "\n",
    "    display(sparkDf.limit(2).toPandas())\n",
    "    return sparkDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf97f",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO WRITE DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a49d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class NumtraConnector:\n",
    "\n",
    "    def put(inStages, inStagesData, stageId, spark, config):\n",
    "        path = eval(config)['server_url']\n",
    "        baseType = eval(config)['baseType']\n",
    "        results_url = eval(config)['results_url']\n",
    "        server = eval(config)['server']\n",
    "        originalfile = eval(config)['orignalKey']\n",
    "        eval(config)['pathOnly']\n",
    "        filename = eval(config)['filename']\n",
    "        eval(config)['ser']\n",
    "        eval(config)['user']\n",
    "        eval(config)['password']\n",
    "        eval(config)['authSource']\n",
    "        eval(config)['user_id']\n",
    "        eval(config)['parent_id']\n",
    "        eval(config)['project_id']\n",
    "        time = str(int(datetime.datetime.now().timestamp()))\n",
    "\n",
    "        inStagesData[inStages[0]]\n",
    "\n",
    "        print(path)\n",
    "        print(baseType)\n",
    "        print(results_url)\n",
    "        print(server)\n",
    "        print(originalfile)\n",
    "        print(filename)\n",
    "\n",
    "        args = {\n",
    "            'url': path,\n",
    "            'baseType': baseType,\n",
    "            'originalfile': originalfile,\n",
    "            'filename': time + filename\n",
    "        }\n",
    "\n",
    "        response = requests.post(results_url, args)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215a099",
   "metadata": {},
   "source": [
    "***READING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CREATE SPARK SESSION ############################ ENTER YOUR SPARK MASTER IP AND PORT TO CONNECT TO SERVER ################\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[1]').getOrCreate()\n",
    "#%run dpHooks.ipynb\n",
    "try:\n",
    "\t#sourcePreExecutionHook()\n",
    "\n",
    "\tairpaengertrain = HDFSConnector.fetch(spark, \"{'url': '/FileStore/platform/testdata/1703617680875_AIRPassengerTrain.csv', 'filename': 'AIRPassengerTrain.csv', 'delimiter': ',', 'file_type': 'Delimeted', 'dbfs_token': '', 'dbfs_domain': '', 'FilePath': '/finance/AIRPassengerTrain.csv', 'viewFileName': 'AIRPassengerTrain.csv', 'is_header': 'Use Header Line', 'baseType': 'hdfs', 'server_url': '/numtraPlatform/NumtraPlatformV3/uploads/platform/', 'results_url': 'http://mluat.numtra.com:44040/api/read/hdfs'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f517c",
   "metadata": {},
   "source": [
    "***PERFORMING OPERATIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run dpHooks.ipynb\n",
    "try:\n",
    "\t#operationPreExecutionHook()\n",
    "\n",
    "datapreparation = runDataCleansing(airpaengertrain,spark,json.dumps( {\"url\": \"/FileStore/platform/testdata/1703617680875_AIRPassengerTrain.csv\", \"source_attributes\": {}, \"DataPrepFile\": \"/FileStore/platform/testdata/1703617680875_AIRPassengerTrain.csv\", \"data_source\": \"platfiles\", \"startListenerOnly\": 1, \"dateColumnNames\": [\"Month\"], \"FilePath\": \"/FileStore/platform/extra/658b22890538aa01e4bb09aa1703619102/0part.csv\", \"requestRatio\": 0.0, \"totalRows\": 144, \"BasicStats\": {\"missingValues\": 0.0, \"numberOfColumns\": 3, \"numberOfRows\": 144, \"duplicateRowCount\": 0, \"stats\": [{\"column\": \"Month\", \"alias\": \"Month\", \"generated\": 0, \"type\": \"date\", \"max\": \"1960121\", \"min\": \"194911\", \"mean\": \"\", \"missing\": 0.0, \"stddev\": \"\", \"outliers\": [], \"validation\": []}, {\"column\": \"#Passengers\", \"alias\": \"#Passengers\", \"generated\": 0, \"type\": \"numeric\", \"max\": 622, \"min\": 104, \"mean\": 280.2986111111111, \"missing\": 0.0, \"stddev\": 119.97, \"outliers\": [622, 606], \"validation\": []}, {\"column\": \"pass_new\", \"alias\": \"pass_new\", \"generated\": 1, \"type\": \"numeric\", \"max\": 622, \"min\": 104, \"mean\": 280.2916666666667, \"missing\": 0.0, \"stddev\": 119.98, \"outliers\": [622, 606], \"validation\": []}]}, \"predictionPowerScore\": [{\"#Passengers\": 1.0, \"Month\": 0.700941751, \"pass_new\": 0.9790024416}, {\"#Passengers\": 0.0, \"Month\": 1.0, \"pass_new\": 0.0}, {\"#Passengers\": 0.9790039062, \"Month\": 0.7008928571, \"pass_new\": 1.0}], \"HasBasicStats\": 1, \"functionsApplied\": [{\"functionName\": \"replaceBy\", \"applyOn\": [{\"columnName\": \"#Passengers\", \"type\": \"numeric\", \"min\": 104, \"max\": 622, \"mean\": 280.3}], \"functionsData\": [{\"replaceType\": \"value\", \"toReplace\": \"112\", \"replaceValueType\": \"byValue\", \"replaceValueCondition\": \"equalsto\", \"ReplaceWith\": \"111\", \"asNewColumn\": 1, \"newColumnName\": \"pass_new\"}]}], \"functionChanges\": [{\"columnName\": \"#Passengers\", \"functionName\": \"Replace Outliers\", \"Type\": \"numeric\", \"Parameters\": [{\"replaceType\": \"value\", \"toReplace\": \"112\", \"replaceValueType\": \"byValue\", \"replaceValueCondition\": \"equalsto\", \"ReplaceWith\": \"111\", \"asNewColumn\": 1, \"newColumnName\": \"pass_new\"}]}], \"fileheader\": [{\"field\": \"Month\", \"alias\": \"Month\", \"generated\": 0, \"position\": 1, \"type\": \"date\"}, {\"field\": \"#Passengers\", \"alias\": \"#Passengers\", \"generated\": 0, \"position\": 2, \"type\": \"numeric\"}, {\"field\": \"pass_new\", \"alias\": \"pass_new\", \"generated\": 1, \"position\": 3, \"type\": \"numeric\"}]}))\n",
    "\t#operationPostExecutionHook(datapreparation)\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0de18a",
   "metadata": {},
   "source": [
    "***WRITING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run dpHooks.ipynb\n",
    "try:\n",
    "\t#sinkPreExecutionHook()\n",
    "\n",
    "\tfinanceoutput = NumtraConnector.fetch(spark, \"{'samplefile': '/FileStore/platform/sampleData/658b22820538aa01e4bb09a6/part-00000-e979d620-7bff-4777-8356-2b547d79a61b-c000.csv', 'samplecount': 144, 'originalcount': 144, 'orignalKey': '/FileStore/platform/testdata/1703617680875_AIRPassengerTrain.csv', 'pathOnly': '/finance', 'project_id': '658ab1460538aa01e4bb065b', 'parent_id': '658ab1460538aa01e4bb065b', 'original_schema': [{'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'Month', 'alias': 'Month', 'type': 'numeric', 'position': '0'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': '#Passengers', 'alias': '#Passengers', 'type': 'numeric', 'position': '1'}], 'actual_schema': [{'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'Month', 'alias': 'Month', 'type': 'numeric', 'position': '0'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': '#Passengers', 'alias': '#Passengers', 'type': 'numeric', 'position': '1'}], 'server': 'https://mluat.numtra.com:443', 'server_url': '/numtraPlatform/NumtraPlatformV3/uploads/platform/', 'delimiter': ',', 'file_type': 'Delimeted', 'filename': 'airDP.csv', 'token': '', 'domain': '', 'is_header': 'Use Header Line', 'url': '/FileStore/platform/uploadedSourceFiles/part-00000-3a5f8921-4cfa-46d1-ad78-3b440a960f1b-c000.csv', 'results_url': 'http://mluat.numtra.com:44040/api/read/hdfs'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
